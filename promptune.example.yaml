# Promptune Configuration
# =======================
#
# Copy this file to 'promptune.yaml' and fill in your model choices.
# API keys go in '.env' file (LiteLLM reads them automatically).
#
# .env example:
#   OPENAI_API_KEY=your-key
#   ANTHROPIC_API_KEY=your-key
#   OLLAMA_API_BASE=http://localhost:11434

# ── LLM Models ──────────────────────────────────────────────
# All model strings use LiteLLM format.
# See: https://docs.litellm.ai/docs/providers
#
# Examples:
#   "gpt-4o-mini"                    → OpenAI
#   "anthropic/claude-3.5-sonnet"    → Anthropic
#   "ollama/llama3.2"                → Ollama (local)
#   "together_ai/meta-llama/Llama-3-70b" → Together AI
#   "groq/llama-3.1-8b-instant"     → Groq

models:
  # The model the prompt is being tuned FOR (runs the prompt + input).
  # This is the model your optimized prompt will be used with in production.
  # The agent itself acts as tuner and judge — no separate models needed.
  target: "gpt-4o-mini"

# ── Optimization Settings ───────────────────────────────────

optimization:
  # Number of examples to randomly sample per test run
  batch_size: 5
